# -*- coding: utf-8 -*-
"""Next_word_prediction_using_LSTM.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/144rJ6U5QCjSFmFPbOLwuzZU2VgXRojCg

# Next Word Prediction using LSTM
Next Word Prediction means predicting the most likely word or phrase that will come next in a sentence or text. It is like having an inbuilt feature on an application that suggests the next word as you type or speak. The Next Word Prediction Models are used in applications like messaging apps, search engines, virtual assistants, and autocorrect features on smartphones.
start by collecting a diverse dataset of text documents,
preprocess the data by cleaning and tokenizing it,
prepare the data by creating input-output pairs,
engineer features such as word embeddings,
select an appropriate model like an LSTM or GPT,
train the model on the dataset while adjusting hyperparameters,
improve the model by experimenting with different techniques and architectures.

*   start by collecting a diverse dataset of text documents
*   preprocess the data by cleaning and tokenizing it


*   preprocess the data by cleaning and tokenizing it
*   prepare the data by creating input-output pairs


*   engineer features such as word embeddings
*   select an appropriate model like an LSTM


*   train the model on the dataset while adjusting hyperparameters
*   improve the model by experimenting with different techniques and architectures
"""

#import libraries
import numpy as np
import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense

# Read the text file
with open('Data.txt', 'r', encoding='utf-8') as file:
    text = file.read()

tokenizer = Tokenizer()  #tokenize the text to create a sequence of words

tokenizer.fit_on_texts([text])

total_words = len(tokenizer.word_index) + 1

total_words

#create input-output pairs by splitting the text into sequences of tokens
#and forming n-grams from the sequences
input_sequences = []
for line in text.split('\n'):
    token_list = tokenizer.texts_to_sequences([line])[0]
    for i in range(1, len(token_list)):
        n_gram_sequence = token_list[:i+1]
        input_sequences.append(n_gram_sequence)  #n-gram sequence represents the input context, with the last token being the target or predicted word

input_sequences

max_sequence_len = max([len(seq) for seq in input_sequences])   #pad the input sequences to have equal length
input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))

input_sequences

# let’s split the sequences into input and output
X = input_sequences[:, :-1]
y = input_sequences[:, -1]

X

y

# let’s convert the output to one-hot encode vectors
y = np.array(tf.keras.utils.to_categorical(y, num_classes=total_words))

y

#let’s build a neural network architecture to train the model
model = Sequential()
model.add(Embedding(total_words, 100, input_length=max_sequence_len-1)) #‘total_words’, which represents the total number of distinct words in the vocabulary
model.add(LSTM(150))              # it will learn 150 internal representations or memory cells
model.add(Dense(total_words, activation='softmax')) # ‘softmax’ activation function to convert the predicted scores into probabilities, indicating the likelihood of each word being the next one in the sequence
print(model.summary())

#let’s compile and train the model
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
model.fit(X, y, epochs=100, verbose=1)

seed_text = "I have mentioned"
next_words = 10

for _ in range(next_words):
    token_list = tokenizer.texts_to_sequences([seed_text])[0]
    token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')
    predicted = np.argmax(model.predict(token_list), axis=-1)
    output_word = ""
    for word, index in tokenizer.word_index.items():
        if index == predicted:
            output_word = word
            break
    seed_text += " " + output_word

print(seed_text)  #code generates the next word predictions based on a given seed text

model.save('LSTM_model.h5')  #HDF5 is a format designed to store large amounts of numerical data, commonly used in machine learning applications for storing trained models

model.save('LSTM.keras')   #this file contains a Keras model saved in HDF5 format

import pickle
with open('tokenizer.pickle', 'wb') as handle:
    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)

print("Tokenizer saved as 'tokenizer.pickle'")

!pip install gradio

!pip install --upgrade gradio

import gradio as gr
import tensorflow as tf
from tensorflow.keras.models import load_model
from tensorflow.keras.preprocessing.sequence import pad_sequences
import pickle

# Load the pre-trained LSTM model
model = load_model('LSTM_model.h5')

# Load the tokenizer used during model training
with open('tokenizer.pickle', 'rb') as handle:
    tokenizer = pickle.load(handle)

# Define parameters
max_sequence_length = 50  # Adjust according to your trained model

# Function to predict next words
def predict_next_words(input_text, num_words_to_predict):
    sequence = tokenizer.texts_to_sequences([input_text])[0]
    predicted_words = []

    for _ in range(num_words_to_predict):
        sequence_padded = pad_sequences([sequence], maxlen=max_sequence_length, padding='pre')
        prediction = model.predict(sequence_padded)
        predicted_word_index = tf.argmax(prediction, axis=-1).numpy()[0]
        predicted_word = tokenizer.index_word.get(predicted_word_index, '')
        predicted_words.append(predicted_word)
        sequence.append(predicted_word_index)
        sequence = sequence[-max_sequence_length:]

    return ' '.join(predicted_words)

# Define Gradio Interface
inputs = gr.Textbox(label="Enter your query", placeholder="Type here")
output_text = gr.Textbox(label="Completed Sentence", interactive=False)

number_of_words = gr.Number(label="Number of words to predict",minimum=1, maximum=20)
number_of_words.value = 4

def predict_and_return(input_text, number_of_words):
    completed_sentence = predict_next_words(input_text, int(number_of_words))
    return completed_sentence

iface = gr.Interface(
    fn=predict_and_return,
    inputs=[inputs, number_of_words],
    outputs=output_text,
    title="Next Word Prediction using LSTM",
    description="Enter a sentence and the number of words to predict, and get the completed sentence."
)

iface.launch()  # Launch the Gradio interface

